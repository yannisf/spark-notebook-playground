{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cassandra and Spark\n",
    "\n",
    "This notebook is meant to serve as a playground for using Cassandra with Spark.\n",
    "\n",
    "\n",
    "## Launch a Cassandra instanse\n",
    "\n",
    "    $ docker run -d --name cassandra_2_1_5 -p9042:9042 cassandra:2.1.5\n",
    "    \n",
    "\n",
    "### Populate Cassandra with sample data\n",
    "\n",
    "First connect to the instance:\n",
    "\n",
    "    $ docker exec -it cassandra_2_1_5 cqlsh\n",
    "    \n",
    "Then execute the following script:\n",
    "\n",
    "```cql\n",
    "create keyspace test_ks with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n",
    "\n",
    "use test_ks;\n",
    "\n",
    "create table test_tbl (\n",
    "    id int,\n",
    "    english text,\n",
    "    symbol text,\n",
    "    primary key (id)\n",
    ");\n",
    "\n",
    "insert into test_ks.test_tbl (id, english, symbol) values (1, 'one', 'xxx');\n",
    "\n",
    "insert into test_ks.test_tbl (id, english, symbol) values (2, 'two', 'yyy');\n",
    "\n",
    "insert into test_ks.test_tbl (id, english, symbol) values (3, 'three', 'zzz');\n",
    "```\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Connector documentation](https://github.com/datastax/spark-cassandra-connector/tree/master/doc)\n",
    "- [Spark RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "\n",
    "**NOTE**: The Cassandra Spark driver is super sensitive to the following:\n",
    "\n",
    "- Java version\n",
    "- Scala version\n",
    "- Spark version\n",
    "- Cassandra server version\n",
    "\n",
    "The following script is know to work well with:\n",
    "\n",
    "- JDK8\n",
    "- Scala 2.12.10\n",
    "- Spark 2.4.4\n",
    "- Cassandra 2.1.5\n",
    "- Driver 2.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/04/25 19:18:30 WARN Utils: Your hostname, ouranos resolves to a loopback address: 127.0.1.1; using 192.168.1.12 instead (on interface wlp2s0)\n",
      "20/04/25 19:18:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "20/04/25 19:18:31 INFO SparkContext: Running Spark version 2.4.4\n",
      "20/04/25 19:18:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/04/25 19:18:31 INFO SparkContext: Submitted application: FunWithCass\n",
      "20/04/25 19:18:31 INFO SecurityManager: Changing view acls to: yannis\n",
      "20/04/25 19:18:31 INFO SecurityManager: Changing modify acls to: yannis\n",
      "20/04/25 19:18:31 INFO SecurityManager: Changing view acls groups to: \n",
      "20/04/25 19:18:31 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/04/25 19:18:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yannis); groups with view permissions: Set(); users  with modify permissions: Set(yannis); groups with modify permissions: Set()\n",
      "20/04/25 19:18:31 INFO Utils: Successfully started service 'sparkDriver' on port 43861.\n",
      "20/04/25 19:18:31 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/04/25 19:18:31 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/04/25 19:18:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/04/25 19:18:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/04/25 19:18:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1d990ce9-7220-4804-a687-63f39a188428\n",
      "20/04/25 19:18:31 INFO MemoryStore: MemoryStore started with capacity 4.0 GB\n",
      "20/04/25 19:18:31 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/04/25 19:18:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/04/25 19:18:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ouranos.fraglab.net:4040\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/jitpack.io/com/github/jupyter/jvm-repr/0.4.0/jvm-repr-0.4.0.jar at spark://ouranos.fraglab.net:43861/jars/jvm-repr-0.4.0.jar with timestamp 1587831512340\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/github/scopt/scopt_2.12/3.7.1/scopt_2.12-3.7.1.jar at spark://ouranos.fraglab.net:43861/jars/scopt_2.12-3.7.1.jar with timestamp 1587831512341\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-interp-api_2.12.10/1.7.4/ammonite-interp-api_2.12.10-1.7.4.jar at spark://ouranos.fraglab.net:43861/jars/ammonite-interp-api_2.12.10-1.7.4.jar with timestamp 1587831512342\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-ops_2.12/1.7.4/ammonite-ops_2.12-1.7.4.jar at spark://ouranos.fraglab.net:43861/jars/ammonite-ops_2.12-1.7.4.jar with timestamp 1587831512342\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-repl-api_2.12.10/1.7.4/ammonite-repl-api_2.12.10-1.7.4.jar at spark://ouranos.fraglab.net:43861/jars/ammonite-repl-api_2.12.10-1.7.4.jar with timestamp 1587831512342\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/ammonite-util_2.12/1.7.4/ammonite-util_2.12-1.7.4.jar at spark://ouranos.fraglab.net:43861/jars/ammonite-util_2.12-1.7.4.jar with timestamp 1587831512342\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/fansi_2.12/0.2.7/fansi_2.12-0.2.7.jar at spark://ouranos.fraglab.net:43861/jars/fansi_2.12-0.2.7.jar with timestamp 1587831512342\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/geny_2.12/0.1.8/geny_2.12-0.1.8.jar at spark://ouranos.fraglab.net:43861/jars/geny_2.12-0.1.8.jar with timestamp 1587831512343\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/os-lib_2.12/0.3.0/os-lib_2.12-0.3.0.jar at spark://ouranos.fraglab.net:43861/jars/os-lib_2.12-0.3.0.jar with timestamp 1587831512343\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/pprint_2.12/0.5.5/pprint_2.12-0.5.5.jar at spark://ouranos.fraglab.net:43861/jars/pprint_2.12-0.5.5.jar with timestamp 1587831512343\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/lihaoyi/sourcecode_2.12/0.1.7/sourcecode_2.12-0.1.7.jar at spark://ouranos.fraglab.net:43861/jars/sourcecode_2.12-0.1.7.jar with timestamp 1587831512343\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/io/get-coursier/interface/0.0.12/interface-0.0.12.jar at spark://ouranos.fraglab.net:43861/jars/interface-0.0.12.jar with timestamp 1587831512343\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.0.0/scala-collection-compat_2.12-2.0.0.jar at spark://ouranos.fraglab.net:43861/jars/scala-collection-compat_2.12-2.0.0.jar with timestamp 1587831512344\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.2.0/scala-xml_2.12-1.2.0.jar at spark://ouranos.fraglab.net:43861/jars/scala-xml_2.12-1.2.0.jar with timestamp 1587831512344\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/interpreter-api_2.12/0.9.1/interpreter-api_2.12-0.9.1.jar at spark://ouranos.fraglab.net:43861/jars/interpreter-api_2.12-0.9.1.jar with timestamp 1587831512344\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/jupyter-api_2.12/0.9.1/jupyter-api_2.12-0.9.1.jar at spark://ouranos.fraglab.net:43861/jars/jupyter-api_2.12-0.9.1.jar with timestamp 1587831512344\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/sh/almond/scala-kernel-api_2.12.10/0.9.1/scala-kernel-api_2.12.10-0.9.1.jar at spark://ouranos.fraglab.net:43861/jars/scala-kernel-api_2.12.10-0.9.1.jar with timestamp 1587831512344\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.local/share/jupyter/kernels/scala212/launcher.jar at spark://ouranos.fraglab.net:43861/jars/launcher.jar with timestamp 1587831512345\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/twitter/jsr166e/1.1.0/jsr166e-1.1.0.jar at spark://ouranos.fraglab.net:43861/jars/jsr166e-1.1.0.jar with timestamp 1587831512345\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/org/joda/joda-convert/1.2/joda-convert-1.2.jar at spark://ouranos.fraglab.net:43861/jars/joda-convert-1.2.jar with timestamp 1587831512345\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar at spark://ouranos.fraglab.net:43861/jars/commons-beanutils-1.9.3.jar with timestamp 1587831512345\n",
      "20/04/25 19:18:32 INFO SparkContext: Added JAR file:/home/yannis/.cache/coursier/v1/https/repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/2.4.3/spark-cassandra-connector_2.12-2.4.3.jar at spark://ouranos.fraglab.net:43861/jars/spark-cassandra-connector_2.12-2.4.3.jar with timestamp 1587831512345\n",
      "20/04/25 19:18:32 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/04/25 19:18:32 INFO Executor: Using REPL class URI: http://127.0.1.1:44971\n",
      "20/04/25 19:18:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41223.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20/04/25 19:18:32 INFO NettyBlockTransferService: Server created on ouranos.fraglab.net:41223\n",
      "20/04/25 19:18:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/04/25 19:18:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ouranos.fraglab.net, 41223, None)\n",
      "20/04/25 19:18:32 INFO BlockManagerMasterEndpoint: Registering block manager ouranos.fraglab.net:41223 with 4.0 GB RAM, BlockManagerId(driver, ouranos.fraglab.net, 41223, None)\n",
      "20/04/25 19:18:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ouranos.fraglab.net, 41223, None)\n",
      "20/04/25 19:18:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ouranos.fraglab.net, 41223, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://ouranos.fraglab.net:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                                                                     \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.cassandra._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.datastax.spark.connector._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4ce5f537\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@786d99d4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.{\n",
    "    `org.apache.spark::spark-sql:2.4.4`,\n",
    "    `com.datastax.spark::spark-cassandra-connector:2.4.3`\n",
    "}\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.cassandra._\n",
    "\n",
    "import com.datastax.spark.connector._\n",
    "\n",
    "val spark = NotebookSparkSession.builder\n",
    "    .master(\"local[4]\")\n",
    "    .appName(\"FunWithCass\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"localhost\")\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlocal_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, english: string ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val local_df = Seq(\n",
    "  (1, \"one\", \"1\", \"I\"),\n",
    "  (2, \"two\", \"2\", \"II\"),\n",
    "  (3, \"three\", \"3\", \"III\"),\n",
    "  (4, \"four\", \"4\", \"IV\"),\n",
    "  (5, \"five\", \"5\", \"V\"),\n",
    ").toDF(\"id\", \"english\", \"arabic\", \"latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- english: string (nullable = true)\n",
      " |-- arabic: string (nullable = true)\n",
      " |-- latin: string (nullable = true)\n",
      "\n",
      "+---+-------+------+-----+\n",
      "| id|english|arabic|latin|\n",
      "+---+-------+------+-----+\n",
      "|  1|    one|     1|    I|\n",
      "|  2|    two|     2|   II|\n",
      "|  3|  three|     3|  III|\n",
      "|  4|   four|     4|   IV|\n",
      "|  5|   five|     5|    V|\n",
      "+---+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_df.printSchema\n",
    "local_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcass_df\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: int, english: string ... 1 more field]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cass_df = spark\n",
    "  .read\n",
    "  .format(\"org.apache.spark.sql.cassandra\")\n",
    "  .options(Map( \"table\" -> \"test_tbl\", \"keyspace\" -> \"test_ks\"))\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- english: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd8.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd8.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    4 / 4\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd8.sc:2</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    5 / 5\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|english|symbol|\n",
      "+---+-------+------+\n",
      "|  1|    one|  null|\n",
      "|  2|    two|  null|\n",
      "|  4|   four|  null|\n",
      "|  5|   five|  null|\n",
      "|  3|  three|  null|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cass_df.printSchema\n",
    "cass_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcass\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mCassandraTableScanRDD\u001b[39m[\u001b[32mCassandraRow\u001b[39m] = CassandraTableScanRDD[6] at RDD at CassandraRDD.scala:19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cass = sc.cassandraTable(\"test_ks\", \"test_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">runJob at RDDFunctions.scala:119</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    10 / 10\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cass.deleteFromCassandra(\"test_ks\", \"test_tbl\", SomeColumns(\"symbol\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">map at RDDFunctions.scala:271</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    2 / 2\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">collect at cmd21.sc:3</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    10 / 10\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres21\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mRow\u001b[39m, \u001b[32mCassandraRow\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  ([1], CassandraRow{id: 1, english: one, symbol: null}),\n",
       "  ([2], CassandraRow{id: 2, english: two, symbol: null})\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_df.selectExpr(\"id\").where(\"id < 3\").rdd\n",
    "    .repartitionByCassandraReplica(\"test_ks\", \"test_tbl\", 10)\n",
    "    .joinWithCassandraTable(\"test_ks\", \"test_tbl\")\n",
    "    .collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val collection = sc.parallelize(Seq((4, \"four\", \"IV\"), (5, \"five\", \"V\")))\n",
    "collection.saveToCassandra(\"test_ks\", \"test_tbl\", SomeColumns(\"id\", \"english\", \"symbol\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
